{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.30.52, Python 3.13.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "sys.path.append(os.path.abspath(\"/home/raggadruid/Documents/ZephyrRL-Training/\"))\n",
    "\n",
    "from src.training_algorithms.reinforce import REINFORCE\n",
    "from src.env.sailboat_env import SailboatEnv\n",
    "from src.utils.plotting import training_reward\n",
    "\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at episode 0\n",
      "Episode 0: Reward -1.69, Avg (last 10): -1.69\n",
      "Estimated Remaining: 1 day, 3:34:55\n",
      "Episode 50: Reward -1.23, Avg (last 10): -3.41\n",
      "Estimated Remaining: 1:22:39\n",
      "Episode 100: Reward -1.92, Avg (last 10): -2.93\n",
      "Estimated Remaining: 1:02:07\n",
      "Episode 150: Reward -1.08, Avg (last 10): -3.01\n",
      "Estimated Remaining: 1:02:22\n",
      "Saved model at episode 200\n",
      "Episode 200: Reward -1.69, Avg (last 10): -0.83\n",
      "Estimated Remaining: 0:59:28\n",
      "Episode 250: Reward 4.99, Avg (last 10): -0.93\n",
      "Estimated Remaining: 0:57:49\n",
      "Episode 300: Reward -3.57, Avg (last 10): -1.58\n",
      "Estimated Remaining: 0:57:08\n",
      "Episode 350: Reward -2.02, Avg (last 10): -2.15\n",
      "Estimated Remaining: 0:55:10\n",
      "Saved model at episode 400\n",
      "Episode 400: Reward -4.13, Avg (last 10): -1.91\n",
      "Estimated Remaining: 0:53:26\n",
      "Episode 450: Reward -3.70, Avg (last 10): -1.84\n",
      "Estimated Remaining: 0:52:30\n",
      "Episode 500: Reward -6.98, Avg (last 10): -0.85\n",
      "Estimated Remaining: 0:51:34\n",
      "Episode 550: Reward -5.24, Avg (last 10): -2.39\n",
      "Estimated Remaining: 0:52:01\n",
      "Saved model at episode 600\n",
      "Episode 600: Reward -2.51, Avg (last 10): -2.02\n",
      "Estimated Remaining: 0:51:05\n",
      "Episode 650: Reward -2.03, Avg (last 10): -2.44\n",
      "Estimated Remaining: 0:51:14\n",
      "Episode 700: Reward 5.14, Avg (last 10): -1.43\n",
      "Estimated Remaining: 0:50:20\n",
      "Episode 750: Reward -5.80, Avg (last 10): -3.95\n",
      "Estimated Remaining: 0:50:26\n",
      "Saved model at episode 800\n",
      "Episode 800: Reward -1.45, Avg (last 10): -2.64\n",
      "Estimated Remaining: 0:50:24\n",
      "Episode 850: Reward -6.72, Avg (last 10): -2.21\n",
      "Estimated Remaining: 0:50:15\n",
      "Episode 900: Reward -1.37, Avg (last 10): -2.31\n",
      "Estimated Remaining: 0:49:52\n",
      "Episode 950: Reward -1.60, Avg (last 10): -2.73\n",
      "Estimated Remaining: 0:48:49\n",
      "Saved model at episode 1000\n",
      "Episode 1000: Reward -2.22, Avg (last 10): -1.57\n",
      "Estimated Remaining: 0:50:50\n",
      "Episode 1050: Reward -1.61, Avg (last 10): -2.37\n",
      "Estimated Remaining: 0:50:42\n",
      "Episode 1100: Reward -2.01, Avg (last 10): -2.71\n",
      "Estimated Remaining: 0:50:41\n",
      "Episode 1150: Reward -1.88, Avg (last 10): -1.71\n",
      "Estimated Remaining: 0:50:05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 32\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     obs, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     34\u001b[0m     agent\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/Documents/ZephyrRL-Training/src/training_algorithms/reinforce.py:103\u001b[0m, in \u001b[0;36mREINFORCE.sample_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    101\u001b[0m distrib \u001b[38;5;241m=\u001b[39m Normal(action_means[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps, action_stddevs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n\u001b[1;32m    102\u001b[0m action \u001b[38;5;241m=\u001b[39m distrib\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m--> 103\u001b[0m prob \u001b[38;5;241m=\u001b[39m \u001b[43mdistrib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mappend(prob)\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torch/distributions/normal.py:91\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     84\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     85\u001b[0m log_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     86\u001b[0m     math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, Real) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m.\u001b[39mlog()\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;241m-\u001b[39m((value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m var)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;241m-\u001b[39m log_scale\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     92\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_path = \"/home/raggadruid/Documents/ZephyrRL-Training/src/model_saves/variable_wind_model.pth\"\n",
    "\n",
    "#Parameters\n",
    "total_episodes = 20000\n",
    "save_interval = 200\n",
    "info_interval = 50\n",
    "draw_interval = 1000\n",
    "\n",
    "\n",
    "\n",
    "wind_settings = {\n",
    "    \"type\": 'variable_per_epoch'\n",
    "}\n",
    "#env = SailboatEnv(**ENV_PARAMS)\n",
    "env = SailboatEnv(wind_settings)\n",
    "\n",
    "obs_space_dims = 8\n",
    "action_space_dims = 1\n",
    "\n",
    "agent = REINFORCE(obs_space_dims, action_space_dims)\n",
    "reward_over_episodes = []\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "# Training loop\n",
    "for episode in range(total_episodes):\n",
    "    episode_start_time = time.time()  # Record episode start time\n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.sample_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        agent.rewards.append(reward)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if episode % draw_interval == 0:  # Render every 500 episodes\n",
    "            time.sleep(1/60)\n",
    "            env.draw()\n",
    "    \n",
    "    reward_over_episodes.append(episode_reward)\n",
    "    agent.update()\n",
    "    \n",
    "    # Calculate time per episode\n",
    "    episode_time = time.time() - episode_start_time  \n",
    "    elapsed_time = time.time() - start_time  \n",
    "\n",
    "    # Estimate total training time\n",
    "    estimated_total_time = (elapsed_time / (episode + 1)) * total_episodes\n",
    "    remaining_time = estimated_total_time - elapsed_time\n",
    "    \n",
    "    if episode % save_interval == 0:\n",
    "        torch.save(agent.net.state_dict(), model_path)\n",
    "        print(f\"Saved model at episode {episode}\")\n",
    "    \n",
    "    if episode % info_interval == 0:\n",
    "        avg_reward = np.mean(reward_over_episodes[-10:])\n",
    "        print(f\"Episode {episode}: Reward {episode_reward:.2f}, Avg (last 10): {avg_reward:.2f}\")\n",
    "        print(f\"Estimated Remaining: {timedelta(seconds=int(remaining_time))}\")\n",
    "        \n",
    "        \n",
    "training_reward(reward_over_episodes, 'Variable Wind Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 10  # Replace with the value you want to use as the threshold\n",
    "filtered_rewards = [reward for reward in reward_over_episodes if abs(reward) < threshold]\n",
    "\n",
    "training_reward(filtered_rewards, 'Variable Wind Training')\n",
    "\n",
    "print(filtered_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
