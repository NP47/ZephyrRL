{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wizard/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.normal import Normal\n",
    "import gymnasium as gym\n",
    "\n",
    "from gymnasium_env.train_test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wizard/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:212: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39msample_action(obs)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Step return type - `tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]`\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# These represent the next observation, the reward from the step,\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# if the episode is terminated, if the episode is truncated and\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# additional info from the step\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     agent\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     40\u001b[0m reward_over_episodes\u001b[38;5;241m.\u001b[39mappend(wrapped_env\u001b[38;5;241m.\u001b[39mreturn_queue[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/common.py:513\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_returns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"gymnasium_env/SailBoat-v0\")\n",
    "\n",
    "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)\n",
    "\n",
    "  # Records episode-reward\n",
    "\n",
    "total_num_episodes = int(5e3)  \n",
    "\n",
    "obs_space_dims = env.observation_space.shape[0]\n",
    "\n",
    "action_space_dims = env.action_space.shape[0]\n",
    "rewards_over_seeds = []\n",
    "\n",
    "for seed in [1, 2, 3, 5, 8]:  # Fibonacci seeds\n",
    "    # set seed\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Reinitialize agent every seed\n",
    "    agent = REINFORCE(obs_space_dims, action_space_dims)\n",
    "    reward_over_episodes = []\n",
    "\n",
    "    for episode in range(total_num_episodes):\n",
    "        # gymnasium requires users to set seed while resetting the environment\n",
    "        obs, info = wrapped_env.reset(seed=seed)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.sample_action(obs)\n",
    "            # Step return type - `tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]`\n",
    "            # These represent the next observation, the reward from the step,\n",
    "            # if the episode is terminated, if the episode is truncated and\n",
    "            # additional info from the step\n",
    "            obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "            agent.rewards.append(reward)\n",
    "\n",
    "            # End the episode when either truncated or terminated is true\n",
    "            #  - truncated: The episode duration reaches max number of timesteps\n",
    "            #  - terminated: Any of the state space values is no longer finite.\n",
    "            done = terminated or truncated or info['in_bounds']\n",
    "\n",
    "        reward_over_episodes.append(wrapped_env.return_queue[-1])\n",
    "        agent.update()\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            avg_reward = int(np.mean(wrapped_env.return_queue))\n",
    "            print(\"Episode:\", episode, \"Average Reward:\", avg_reward)\n",
    "\n",
    "    rewards_over_seeds.append(reward_over_episodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_to_plot = [[reward[0] for reward in rewards] for rewards in rewards_over_seeds]\n",
    "df1 = pd.DataFrame(rewards_to_plot).melt()\n",
    "df1.rename(columns={\"variable\": \"episodes\", \"value\": \"reward\"}, inplace=True)\n",
    "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\")\n",
    "sns.lineplot(x=\"episodes\", y=\"reward\", data=df1).set(\n",
    "    title=\"REINFORCE for InvertedPendulum-v4\"\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
